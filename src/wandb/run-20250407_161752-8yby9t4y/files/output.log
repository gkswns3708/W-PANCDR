Hyperparameters:
nz: 128
d_dim: 128
lr: 0.001
lr_critic: 0.0001
lam: 0.01
batch_size: [256, 28]
device: cuda
TX_drug_feat_data_test : cuda:0
TX_drug_adj_data_test  : cuda:0
TX_gexpr_data_test     : cuda:0
TY_test                : cuda:0
Epoch:   0%|                                                                                                                                                                                                     | 0/1000 [00:24<?, ?it/s]
Traceback (most recent call last):                                                                                                                                                                                                        
  File "run_W_PANCDR_nested.py", line 83, in <module>
    auc_test_df = train_W_PANCDR_nested(n_outer_splits,data,best_params_file)
  File "/mnt/W-PANCDR/src/ModelTraining/W_PANCDR.py", line 273, in train_W_PANCDR_nested
    auc_TEST, current_epoch = model.train(current_params, weight_path=f'../checkpoint/GDSC_kfold/model_best_outerfold_{outer_fold}_{i}.pt')
  File "/mnt/W-PANCDR/src/ModelTraining/W_PANCDR.py", line 156, in train
    y_pred_val = GCN_model(X_drug_feat_val, X_drug_adj_val, F_val)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1423, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/W-PANCDR/src/ModelTraining/model.py", line 276, in forward
    GCN_layer = self.BRD1(self.GC1([drug_feat, drug_adj])).permute(0,2,1)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1423, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/W-PANCDR/src/ModelTraining/model.py", line 40, in forward
    return self._forward(input, self.weight, self.bias)
  File "/mnt/W-PANCDR/src/ModelTraining/model.py", line 33, in _forward
    outputs = torch.matmul(features, weight) + bias
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 496.00 MiB (GPU 0; 23.65 GiB total capacity; 8.18 GiB already allocated; 93.12 MiB free; 8.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
